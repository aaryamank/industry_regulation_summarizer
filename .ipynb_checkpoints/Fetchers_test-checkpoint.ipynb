{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcb6072a-cf76-47fc-b0bb-2ce2fde3c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === fetchers.py ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "import streamlit as st\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# Define 3-month cutoff\n",
    "cutoff_date = datetime.today() - timedelta(days=90)\n",
    "\n",
    "def extract_date_from_dpiit_url(pdf_url):\n",
    "    \"\"\"\n",
    "    Attempts to extract a date from a DPIIT PDF URL such as:\n",
    "    - https://dpiit.gov.in/sites/default/files/QCO_LaboratoryGlassware_24January2024.pdf\n",
    "    - https://dpiit.gov.in/sites/default/files/notification_Amendment_23November2012%20%206_0.pdf\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "\n",
    "    # Extract the filename\n",
    "    filename = pdf_url.split('/')[-1]\n",
    "    \n",
    "    # Remove extension and decode URL\n",
    "    filename = re.sub(r'\\.pdf$', '', filename, flags=re.IGNORECASE)\n",
    "    filename = re.sub(r'%20', ' ', filename)\n",
    "\n",
    "    # Try to find a pattern like 24January2024 or 23November2012\n",
    "    match = re.search(r'(\\d{1,2})\\s*([A-Za-z]+)\\s*(\\d{4})', filename)\n",
    "    if match:\n",
    "        day, month_str, year = match.groups()\n",
    "        try:\n",
    "            date_obj = datetime.strptime(f\"{day} {month_str} {year}\", \"%d %B %Y\")\n",
    "            return date_obj\n",
    "        except ValueError:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(f\"{day} {month_str} {year}\", \"%d %b %Y\")\n",
    "                return date_obj\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str.strip(), \"%d/%m/%Y\")\n",
    "    except:\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), \"%d-%m-%Y\")\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "def parse_date_string(date_str):\n",
    "    # Try parsing formats like \"May 16, 2025\"\n",
    "    for fmt in (\"%B %d, %Y\", \"%b %d, %Y\", \"%d %B %Y\", \"%d-%m-%Y\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def clean_commerce_date(date_str):\n",
    "    # Remove ordinal suffixes (st, nd, rd, th)\n",
    "    cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str, flags=re.IGNORECASE)\n",
    "    # Replace dots with spaces\n",
    "    cleaned = cleaned.replace('.', ' ')\n",
    "    # Normalize whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned.title()  # Ensures consistent capitalization for parsing\n",
    "\n",
    "# 1. DPIIT\n",
    "def scrape_dpiit():\n",
    "    url = \"https://dpiit.gov.in/policies-rules-and-acts/notifications\"\n",
    "    base_url = \"https://dpiit.gov.in\"\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    results = []\n",
    "\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            link_tag = row.find(\"a\", href=True)\n",
    "            if link_tag and link_tag['href'].endswith(\".pdf\"):\n",
    "                pdf_url = urljoin(base_url, link_tag[\"href\"])\n",
    "                doc_date = extract_date_from_dpiit_url(pdf_url)\n",
    "\n",
    "                if doc_date and doc_date >= cutoff_date:\n",
    "                    title = link_tag.text.strip()\n",
    "                    results.append({\n",
    "                        \"source\": \"DPIIT\",\n",
    "                        \"title\": title,\n",
    "                        \"url\": pdf_url,\n",
    "                        \"date\": doc_date.strftime(\"%Y-%m-%d\")\n",
    "                    })\n",
    "\n",
    "    return results\n",
    "\n",
    "# 2. Power Ministry\n",
    "def scrape_powermin():\n",
    "    base_url = \"https://powermin.gov.in\"\n",
    "    url = \"https://powermin.gov.in/circular\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    results = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 5:\n",
    "                subject = cols[1].get_text(strip=True)\n",
    "                date_text = cols[2].get_text(strip=True)\n",
    "                link_tag = cols[4].find(\"a\", href=True)\n",
    "                doc_date = parse_date(date_text)\n",
    "                if doc_date and doc_date >= cutoff_date and link_tag:\n",
    "                    pdf_url = urljoin(base_url, link_tag['href'])\n",
    "                    results.append({\"source\": \"Power Ministry\", \"title\": subject, \"url\": pdf_url, \"date\": doc_date.strftime(\"%Y-%m-%d\")})\n",
    "    return results\n",
    "\n",
    "# 3. RBI\n",
    "def scrape_rbi():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(\"https://website.rbi.org.in/web/rbi/notifications?delta=100\")\n",
    "\n",
    "    # Scroll to load all content\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    results = []\n",
    "    base_url = \"https://website.rbi.org.in\"\n",
    "    items = soup.find_all(\"div\", class_=\"notification-row-each-inner\")\n",
    "    print(f\"🔎 Notifications found: {len(items)}\")\n",
    "\n",
    "    for block in items:\n",
    "        a_tag = block.find(\"a\", class_=\"mtm_list_item_heading\")\n",
    "        title = a_tag.get_text(strip=True) if a_tag else \"Untitled\"\n",
    "\n",
    "        date_tag = block.find(\"div\", class_=\"notification-date\")\n",
    "        date_str = date_tag.get_text(strip=True) if date_tag else \"\"\n",
    "        doc_date = parse_date_string(date_str)\n",
    "\n",
    "        pdf_tag = block.find(\"a\", class_=\"matomo_download download_link\", href=True)\n",
    "        pdf_url = urljoin(base_url, pdf_tag[\"href\"]) if pdf_tag else None\n",
    "\n",
    "        if doc_date and doc_date >= cutoff_date and pdf_url:\n",
    "            results.append((\n",
    "                \"RBI (New)\",  # string label, not dictionary key\n",
    "                title,\n",
    "                pdf_url,\n",
    "                doc_date.strftime(\"%Y-%m-%d\")\n",
    "            ))\n",
    "\n",
    "    print(f\"✅ Final RBI Notifications with PDFs: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "# 4. Commerce\n",
    "def scrape_commerce():\n",
    "    url = \"https://commerce.gov.in/acts-and-schemes/\"\n",
    "    res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    cards = soup.select(\".whats-new-wrapper\")\n",
    "    results = []\n",
    "\n",
    "    for card in cards:\n",
    "        heading = card.select_one(\"h3\")\n",
    "        meta = card.select_one(\"p\")\n",
    "        link = card.select_one(\"a.innr-btn\")\n",
    "        if not heading or not meta or not link:\n",
    "            continue\n",
    "\n",
    "        title = heading.get_text(strip=True)\n",
    "        raw_date = meta.get_text(strip=True).split(\"|\")[0].strip()\n",
    "\n",
    "        # Clean and normalize date string\n",
    "        normalized_date = clean_commerce_date(raw_date)\n",
    "\n",
    "        # Try multiple formats\n",
    "        doc_date = None\n",
    "        for fmt in (\"%d %B %Y\", \"%d %b %Y\"):\n",
    "            try:\n",
    "                doc_date = datetime.strptime(normalized_date, fmt)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if not doc_date:\n",
    "            continue\n",
    "\n",
    "        pdf_url = link.get(\"href\")\n",
    "        if not pdf_url or \".pdf\" not in pdf_url.lower():\n",
    "            continue\n",
    "        if doc_date < cutoff_date:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"source\": \"Commerce\",\n",
    "            \"title\": title,\n",
    "            \"url\": pdf_url,\n",
    "            \"date\": doc_date.strftime(\"%Y-%m-%d\")\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "87ebde49-56bf-4848-a6e6-e758d0675108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Run & Test ===\n",
    "# from datetime import datetime, timedelta\n",
    "# all_results = []\n",
    "# for scraper in [scrape_dpiit, scrape_powermin, scrape_rbi, scrape_commerce]:\n",
    "#     try:\n",
    "#         all_results += scraper()\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error in {scraper.__name__}: {e}\")\n",
    "\n",
    "# df = pd.DataFrame(all_results)\n",
    "# print(f\"\\n✅ Total documents fetched: {len(df)}\")\n",
    "\n",
    "# if not df.empty:\n",
    "#     print(\"\\n📊 Document counts by ministry:\")\n",
    "#     print(df['source'].value_counts())\n",
    "\n",
    "#     print(\"\\n🔍 Sample entries:\")\n",
    "#     for source in df['source'].unique():\n",
    "#         sample = df[df['source'] == source].head(1)\n",
    "#         print(f\"\\n--- {source} ---\")\n",
    "#         print(sample[['date', 'title', 'url']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a3da535-910f-4ce1-977c-5e9434438bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DPIIT: 4 results\n",
      "\n",
      "✅ Power Ministry: 1 results\n",
      "🔎 Notifications found: 100\n",
      "✅ Final RBI Notifications with PDFs: 39\n",
      "\n",
      "✅ RBI: 39 results\n",
      "\n",
      "✅ Commerce: 0 results\n",
      "\n",
      "🧾 Final DataFrame shape: (44, 4)\n",
      "📋 Columns: ['source', 'title', 'url', 'date']\n",
      "\n",
      "🔍 Missing values per column:\n",
      "source    0\n",
      "title     0\n",
      "url       0\n",
      "date      0\n",
      "dtype: int64\n",
      "\n",
      "📊 Document counts by ministry:\n",
      "source\n",
      "RBI (New)         39\n",
      "DPIIT              4\n",
      "Power Ministry     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔍 Sample entries:\n",
      "           source                                           title  \\\n",
      "0           DPIIT                                       (1.67 MB)   \n",
      "1           DPIIT                                      (832.3 KB)   \n",
      "2           DPIIT                                       (1.96 MB)   \n",
      "3           DPIIT                                       (1.84 MB)   \n",
      "4  Power Ministry  Sexual Harassment Internal Complaint Committee   \n",
      "\n",
      "                                                 url        date  \n",
      "0  https://dpiit.gov.in/sites/default/files/notif...  2025-05-13  \n",
      "1  https://dpiit.gov.in/sites/default/files/QCO_H...  2025-03-26  \n",
      "2  https://dpiit.gov.in/sites/default/files/QCO_C...  2025-02-27  \n",
      "3  https://dpiit.gov.in/sites/default/files/revis...  2025-03-24  \n",
      "4  https://powermin.gov.in/sites/default/files/Se...  2025-02-19  \n"
     ]
    }
   ],
   "source": [
    "# Combine results\n",
    "all_results = []\n",
    "\n",
    "# Scrape each source\n",
    "dpiit = scrape_dpiit()\n",
    "print(f\"\\n✅ DPIIT: {len(dpiit)} results\")\n",
    "all_results.extend(dpiit)\n",
    "\n",
    "powermin = scrape_powermin()\n",
    "print(f\"\\n✅ Power Ministry: {len(powermin)} results\")\n",
    "all_results.extend(powermin)\n",
    "\n",
    "rbi = scrape_rbi()\n",
    "# Note: RBI result is a list of tuples, we convert to dict for consistency\n",
    "rbi_dicts = [{\"source\": src, \"title\": title, \"url\": url, \"date\": date} for (src, title, url, date) in rbi]\n",
    "print(f\"\\n✅ RBI: {len(rbi_dicts)} results\")\n",
    "all_results.extend(rbi_dicts)\n",
    "\n",
    "commerce = scrape_commerce()\n",
    "print(f\"\\n✅ Commerce: {len(commerce)} results\")\n",
    "all_results.extend(commerce)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show DataFrame shape and column names\n",
    "print(f\"\\n🧾 Final DataFrame shape: {df.shape}\")\n",
    "print(f\"📋 Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n🔍 Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Show counts by source\n",
    "if 'source' in df.columns:\n",
    "    print(\"\\n📊 Document counts by ministry:\")\n",
    "    print(df['source'].value_counts())\n",
    "\n",
    "# Show sample entries\n",
    "print(\"\\n🔍 Sample entries:\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff0c14-4945-42a7-8f79-56c8d3dc9fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
